{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller: Construcción e implementación de árboles de decisión y métodos de ensamblaje\n",
    "\n",
    "En este taller podrá poner en práctica los sus conocimientos sobre construcción e implementación de árboles de decisión y métodos de ensamblajes. El taller está constituido por 9 puntos, 5 relacionados con árboles de decisión (parte A) y 4 con métodos de ensamblaje (parte B)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte A - Árboles de decisión\n",
    "\n",
    "En esta parte del taller se usará el conjunto de datos de Capital Bikeshare de Kaggle, donde cada observación representa el alquiler de bicicletas durante una hora y día determinado. Para más detalles puede visitar los siguientes enlaces: [datos](https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip), [dicccionario de datos](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos prestamo de bicicletas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>total</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-01 00:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 01:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 02:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 03:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-01 04:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     season  holiday  workingday  weather  temp   atemp  \\\n",
       "datetime                                                                  \n",
       "2011-01-01 00:00:00       1        0           0        1  9.84  14.395   \n",
       "2011-01-01 01:00:00       1        0           0        1  9.02  13.635   \n",
       "2011-01-01 02:00:00       1        0           0        1  9.02  13.635   \n",
       "2011-01-01 03:00:00       1        0           0        1  9.84  14.395   \n",
       "2011-01-01 04:00:00       1        0           0        1  9.84  14.395   \n",
       "\n",
       "                     humidity  windspeed  casual  registered  total  hour  \n",
       "datetime                                                                   \n",
       "2011-01-01 00:00:00        81        0.0       3          13     16     0  \n",
       "2011-01-01 01:00:00        80        0.0       8          32     40     1  \n",
       "2011-01-01 02:00:00        80        0.0       5          27     32     2  \n",
       "2011-01-01 03:00:00        75        0.0       3          10     13     3  \n",
       "2011-01-01 04:00:00        75        0.0       0           1      1     4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura de la información de archivo .csv\n",
    "bikes = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2023/main/datasets/bikeshare.csv', index_col='datetime', parse_dates=True)\n",
    "\n",
    "# Renombrar variable \"count\" a \"total\"\n",
    "bikes.rename(columns={'count':'total'}, inplace=True)\n",
    "\n",
    "# Crear la hora como una variable \n",
    "bikes['hour'] = bikes.index.hour\n",
    "\n",
    "# Visualización de los datos\n",
    "bikes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 1 - Análisis descriptivo\n",
    "\n",
    "Ejecute las celdas 1.1 y 1.2. A partir de los resultados realice un análisis descriptivo sobre las variables \"season\" y \"hour\", escriba sus inferencias sobre los datos. Para complementar su análisis puede usar métricas como máximo, mínimo, percentiles entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1.1\n",
    "bikes.groupby('season').total.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que las estaciones 3 (verano) y 2 (primavera) son las que presentan los promedios mas altos en el total de alquiler de bicicleta, seguidos por la estación 4 (otoño) y finalmente la estación 1 (invierno). Esto podría sugerir una relación positiva entre la temporada de clima más cálido y el aumento en el alquiler de bicicletas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bikes.groupby('season')['total'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La estación 3 (verano) tiene una mediana más alta y un rango intercuartílico más amplio en comparación con las otras estaciones, lo que sugiere una distribución más amplia de la demanda durante el verano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Celda 1.2\n",
    "bikes.groupby('hour').total.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bikes.groupby('hour')['total'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se observa que las horas que presentan mayor demanda en las horas de la mañana son de 7:00 a.m. a 9:00 a.m. y en la tarde de 4:00 p.m. a 7:00 p.m.\n",
    "* Las horas con la menor demanda de alquiler de bicicletas son durante la noche y las primeras horas de la madrugada, desde la medianoche hasta las 6:00 a.m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 2 - Análisis de gráficos\n",
    "\n",
    "Primero ejecute la celda 2.1 y asegúrese de comprender el código y el resultado. Luego, en cada una de celdas 2.2 y 2.3 escriba un código que genere una gráfica del número de bicicletas rentadas promedio para cada valor de la variable \"hour\" (hora) cuando la variable \"season\" es igual a 1 (invierno) e igual a 3 (verano), respectivamente. Analice y escriba sus hallazgos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2.1 - rentas promedio para cada valor de la variable \"hour\"\n",
    "bikes.groupby('hour').total.mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2.2 - \"season\"=1 escriba su código y hallazgos \n",
    "bikes_1 = bikes[bikes['season'] == 1]\n",
    "bikes_1.groupby('hour').total.mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que al ser la estación de invierno, los promedios generales de alquiler de bicicletas disminuyen en comparación con el total general, sin hacer distinción por estación. Sin embargo, se evidencian picos de alquiler en las mismas franjas horarias, que van entre las 7:00 a.m. y las 9:00 a.m., así como entre las 4:00 p.m. y las 7:00 p.m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2.3 - \"season\"=3 escriba su código y hallazgos \n",
    "bikes_3 = bikes[bikes['season'] == 3]\n",
    "bikes_3.groupby('hour').total.mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera contraria se observa que para esta estación que corresponde a la de verano, los promedios totales de alquiler de bicicletas están por encima del promedio general. Además, se evidencian picos en las mismas franjas horarias, \n",
    "por lo que se puede inferir que independientemente de la estación, se observan picos altos de alquiler en las mismas horas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 3 - Regresión lineal\n",
    "En la celda 3 ajuste un modelo de regresión lineal a todo el conjunto de datos, utilizando \"total\" como variable de respuesta y \"season\" y \"hour\" como las únicas variables predictoras, teniendo en cuenta que la variable \"season\" es categórica. Luego, imprima los coeficientes e interprételos. ¿Cuáles son las limitaciones de la regresión lineal en este caso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes:\n",
      "hour: 10.545206094069927\n",
      "season_2: 100.31723191606622\n",
      "season_3: 119.46754994593317\n",
      "season_4: 84.08311787296769\n",
      "\n",
      "Intercepto: -6.430262462306786\n"
     ]
    }
   ],
   "source": [
    "# Celda 3\n",
    "bikes_punto3 = pd.get_dummies(bikes, columns=['season'], drop_first=True)\n",
    "\n",
    "X = bikes_punto3[['hour', 'season_2', 'season_3', 'season_4']]\n",
    "y = bikes_punto3['total']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"Coeficientes:\")\n",
    "for coef, feature_name in zip(model.coef_, X.columns):\n",
    "    print(f\"{feature_name}: {coef}\")\n",
    "\n",
    "print(\"\\nIntercepto:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Con respecto al coeficiente de la hora se puede decir que manteniendo todas las demás variables constantes, por cada aumento de una unidad en la hora del día, se espera un aumento de aproximadamente 10.55 en el número total de bicicletas alquiladas. Esto sugiere que el número de bicicletas alquiladas tiende a aumentar a medida que avanza el día.\n",
    "* Con respecto a la estación 2 (primavera) se espera que haya aproximadamente 100.32 más bicicletas alquiladas que durante la estación 1 (invierno), manteniendo todas las demás variables constantes.\n",
    "* Para la estación 3 (verano) se espera que haya aproximadamente 119.46 más bicicletas alquiladas que durante la estación 1 (invierno), manteniendo todas las demás variables constantes.\n",
    "* Por ultimo para la estación 4 (primavera) se espera que haya aproximadamente 84.08 más bicicletas alquiladas que durante la estación 1 (invierno), manteniendo todas las demás variables constantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 ajustado: 0.2231079081517925\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calcula el R^2\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "# Número de observaciones\n",
    "n = len(y)\n",
    "\n",
    "# Número de variables predictoras\n",
    "p = X.shape[1]\n",
    "\n",
    "# Calcula el R^2 ajustado\n",
    "r2_adjusted_punto3 = 1 - (1 - r2) * ((n - 1) / (n - p - 1))\n",
    "\n",
    "print(\"R^2 ajustado:\", r2_adjusted_punto3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitaciones de la regresión lineal**\n",
    "* La regresión lineal asume que la relación entre las variables predictoras y la variable de respuesta es lineal.por lo que en este caso, la relación entre la hora del día y el número total de bicicletas alquiladas puede no ser necesariamente lineal en todos los casos. Por ejemplo, es posible que la demanda de alquiler de bicicletas aumente de manera no lineal durante las horas pico.\n",
    "*  La regresión lineal puede ser sensible a valores atípicos en los datos. Si hay valores atípicos presentes en el conjunto de datos, pueden afectar los coeficientes estimados y la precisión del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 4 - Árbol de decisión manual\n",
    "En la celda 4 cree un árbol de decisiones para pronosticar la variable \"total\" iterando **manualmente** sobre las variables \"hour\" y  \"season\". El árbol debe tener al menos 6 nodos finales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4\n",
    "# Definir parámetros y criterios de parada\n",
    "X = bikes[['hour', 'season']]\n",
    "y = bikes['total']\n",
    "\n",
    "max_depth = None\n",
    "num_pct = 10\n",
    "max_features = None\n",
    "min_gain=0.001\n",
    "\n",
    "# Definición de la función que calcula el gini index\n",
    "def gini(y):\n",
    "    if y.shape[0] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 - (y.mean()**2 + (1 - y.mean())**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular la ganancia de Gini para una división\n",
    "def gini_impurity(X_col, y, split):\n",
    "    \n",
    "    filter_l = X_col < split\n",
    "    y_l = y.loc[filter_l]\n",
    "    y_r = y.loc[~filter_l]\n",
    "    \n",
    "    n_l = y_l.shape[0]\n",
    "    n_r = y_r.shape[0]\n",
    "    \n",
    "    gini_y = gini(y)\n",
    "    gini_l = gini(y_l)\n",
    "    gini_r = gini(y_r)\n",
    "    \n",
    "    gini_impurity_ = gini_y - (n_l / (n_l + n_r) * gini_l + n_r / (n_l + n_r) * gini_r)\n",
    "    \n",
    "    return gini_impurity_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para encontrar la mejor división para una variable\n",
    "def best_split(X, y, num_pct=10):\n",
    "    \n",
    "    features = range(X.shape[1])\n",
    "    \n",
    "    best_split = [0, 0, 0]  # j, split, gain\n",
    "    \n",
    "    # Para todas las varibles \n",
    "    for j in features:\n",
    "        \n",
    "        splits = np.percentile(X.iloc[:, j], np.arange(0, 100, 100.0 / (num_pct+1)).tolist())\n",
    "        splits = np.unique(splits)[1:]\n",
    "        \n",
    "        # Para cada partición\n",
    "        for split in splits:\n",
    "            gain = gini_impurity(X.iloc[:, j], y, split)\n",
    "                        \n",
    "            if gain > best_split[2]:\n",
    "                best_split = [j, split, gain]\n",
    "    \n",
    "    return best_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_grow(X, y, level=0, min_gain=0.001, max_depth=None, num_pct=10):\n",
    "    \n",
    "    # Si solo es una observación\n",
    "    if X.shape[0] == 1:\n",
    "        tree = dict(y_pred=y.iloc[:1].values[0], y_prob=0.5, level=level, split=-1, n_samples=1, gain=0)\n",
    "        return tree\n",
    "    \n",
    "    # Calcular la mejor división\n",
    "    j, split, gain = best_split(X, y, num_pct)\n",
    "    \n",
    "    # Guardar el árbol y estimar la predicción\n",
    "    y_pred = int(y.mean() >= 0.5) \n",
    "    y_prob = (y.sum() + 1.0) / (y.shape[0] + 2.0)  # Corrección Laplace \n",
    "    \n",
    "    tree = dict(y_pred=y_pred, y_prob=y_prob, level=level, split=-1, n_samples=X.shape[0], gain=gain)\n",
    "    # Revisar el criterio de parada \n",
    "    if gain < min_gain:\n",
    "        return tree\n",
    "    if max_depth is not None:\n",
    "        if level >= max_depth:\n",
    "            return tree   \n",
    "    \n",
    "    # Continuar creando la partición\n",
    "    filter_l = X.iloc[:, j] < split\n",
    "    X_l, y_l = X.loc[filter_l], y.loc[filter_l]\n",
    "    X_r, y_r = X.loc[~filter_l], y.loc[~filter_l]\n",
    "    tree['split'] = [j, split]\n",
    "\n",
    "    # Siguiente iteración para cada partición\n",
    "    \n",
    "    tree['sl'] = tree_grow(X_l, y_l, level + 1, min_gain=min_gain, max_depth=max_depth, num_pct=num_pct)\n",
    "    tree['sr'] = tree_grow(X_r, y_r, level + 1, min_gain=min_gain, max_depth=max_depth, num_pct=num_pct)\n",
    "    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y_pred': 1,\n",
       " 'y_prob': 191.53903379867745,\n",
       " 'level': 0,\n",
       " 'split': [0, 8.0],\n",
       " 'n_samples': 10886,\n",
       " 'gain': 18268.811823533004,\n",
       " 'sl': {'y_pred': 1,\n",
       "  'y_prob': 55.40711902113459,\n",
       "  'level': 1,\n",
       "  'split': [0, 7.0],\n",
       "  'n_samples': 3594,\n",
       "  'gain': 7207.700659959655,\n",
       "  'sl': {'y_pred': 1,\n",
       "   'y_prob': 32.561604584527224,\n",
       "   'level': 2,\n",
       "   'split': [0, 6.0],\n",
       "   'n_samples': 3139,\n",
       "   'gain': 646.8008927589567,\n",
       "   'sl': {'y_pred': 1,\n",
       "    'y_prob': 25.15934475055845,\n",
       "    'level': 3,\n",
       "    'split': [0, 2.0],\n",
       "    'n_samples': 2684,\n",
       "    'gain': 382.8088308604629,\n",
       "    'sl': {'y_pred': 1,\n",
       "     'y_prob': 44.41383095499451,\n",
       "     'level': 4,\n",
       "     'split': [1, 2.0],\n",
       "     'n_samples': 909,\n",
       "     'gain': 291.4004681825131,\n",
       "     'sl': {'y_pred': 1,\n",
       "      'y_prob': 23.32456140350877,\n",
       "      'level': 5,\n",
       "      'split': [0, 1.0],\n",
       "      'n_samples': 226,\n",
       "      'gain': 45.41972746495435,\n",
       "      'sl': {'y_pred': 1,\n",
       "       'y_prob': 27.808695652173913,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 113,\n",
       "       'gain': 0},\n",
       "      'sr': {'y_pred': 1,\n",
       "       'y_prob': 18.443478260869565,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 113,\n",
       "       'gain': 0}},\n",
       "     'sr': {'y_pred': 1,\n",
       "      'y_prob': 51.3051094890511,\n",
       "      'level': 5,\n",
       "      'split': [0, 1.0],\n",
       "      'n_samples': 683,\n",
       "      'gain': 316.1751143186211,\n",
       "      'sl': {'y_pred': 1,\n",
       "       'y_prob': 63.63953488372093,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 342,\n",
       "       'gain': 37.59626038780971},\n",
       "      'sr': {'y_pred': 1,\n",
       "       'y_prob': 38.63848396501458,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 341,\n",
       "       'gain': 10.425846423569055}}},\n",
       "    'sr': {'y_pred': 1,\n",
       "     'y_prob': 15.260551491277434,\n",
       "     'level': 4,\n",
       "     'split': [0, 3.0],\n",
       "     'n_samples': 1775,\n",
       "     'gain': 39.229899273669275,\n",
       "     'sl': {'y_pred': 1,\n",
       "      'y_prob': 22.8,\n",
       "      'level': 5,\n",
       "      'split': [1, 2.0],\n",
       "      'n_samples': 448,\n",
       "      'gain': 58.9740013202146,\n",
       "      'sl': {'y_pred': 1,\n",
       "       'y_prob': 12.972477064220184,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 107,\n",
       "       'gain': 0},\n",
       "      'sr': {'y_pred': 1,\n",
       "       'y_prob': 25.793002915451893,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 341,\n",
       "       'gain': 10.65568633118778}},\n",
       "     'sr': {'y_pred': 1,\n",
       "      'y_prob': 12.685477802859292,\n",
       "      'level': 5,\n",
       "      'split': [0, 5.0],\n",
       "      'n_samples': 1327,\n",
       "      'gain': 51.55182386612603,\n",
       "      'sl': {'y_pred': 1,\n",
       "       'y_prob': 9.035347776510832,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 875,\n",
       "       'gain': 14.311158670464266},\n",
       "      'sr': {'y_pred': 1,\n",
       "       'y_prob': 19.68281938325991,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 452,\n",
       "       'gain': 57.661438120649336}}}},\n",
       "   'sr': {'y_pred': 1,\n",
       "    'y_prob': 75.92778993435448,\n",
       "    'level': 3,\n",
       "    'split': [1, 2.0],\n",
       "    'n_samples': 455,\n",
       "    'gain': 743.095592335012,\n",
       "    'sl': {'y_pred': 1,\n",
       "     'y_prob': 41.99130434782609,\n",
       "     'level': 4,\n",
       "     'split': -1,\n",
       "     'n_samples': 113,\n",
       "     'gain': 0},\n",
       "    'sr': {'y_pred': 1,\n",
       "     'y_prob': 86.83430232558139,\n",
       "     'level': 4,\n",
       "     'split': [1, 4.0],\n",
       "     'n_samples': 342,\n",
       "     'gain': 25.855143462948035,\n",
       "     'sl': {'y_pred': 1,\n",
       "      'y_prob': 89.10434782608695,\n",
       "      'level': 5,\n",
       "      'split': [1, 3.0],\n",
       "      'n_samples': 228,\n",
       "      'gain': 95.19579101261843,\n",
       "      'sl': {'y_pred': 1,\n",
       "       'y_prob': 81.5603448275862,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 114,\n",
       "       'gain': 0},\n",
       "      'sr': {'y_pred': 1,\n",
       "       'y_prob': 95.12068965517241,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 114,\n",
       "       'gain': 0}},\n",
       "     'sr': {'y_pred': 1,\n",
       "      'y_prob': 80.84482758620689,\n",
       "      'level': 5,\n",
       "      'split': -1,\n",
       "      'n_samples': 114,\n",
       "      'gain': 0}}}},\n",
       "  'sr': {'y_pred': 1,\n",
       "   'y_prob': 212.18599562363238,\n",
       "   'level': 2,\n",
       "   'split': [1, 2.0],\n",
       "   'n_samples': 455,\n",
       "   'gain': 5197.1071057533845,\n",
       "   'sl': {'y_pred': 1,\n",
       "    'y_prob': 122.27826086956522,\n",
       "    'level': 3,\n",
       "    'split': -1,\n",
       "    'n_samples': 113,\n",
       "    'gain': 0},\n",
       "   'sr': {'y_pred': 1,\n",
       "    'y_prob': 241.01162790697674,\n",
       "    'level': 3,\n",
       "    'split': [1, 4.0],\n",
       "    'n_samples': 342,\n",
       "    'gain': 131.37717588317173,\n",
       "    'sl': {'y_pred': 1,\n",
       "     'y_prob': 245.99565217391304,\n",
       "     'level': 4,\n",
       "     'split': [1, 3.0],\n",
       "     'n_samples': 228,\n",
       "     'gain': 504.7277623884438,\n",
       "     'sl': {'y_pred': 1,\n",
       "      'y_prob': 228.26724137931035,\n",
       "      'level': 5,\n",
       "      'split': -1,\n",
       "      'n_samples': 114,\n",
       "      'gain': 0},\n",
       "     'sr': {'y_pred': 1,\n",
       "      'y_prob': 259.4913793103448,\n",
       "      'level': 5,\n",
       "      'split': -1,\n",
       "      'n_samples': 114,\n",
       "      'gain': 0}},\n",
       "    'sr': {'y_pred': 1,\n",
       "     'y_prob': 226.98275862068965,\n",
       "     'level': 4,\n",
       "     'split': -1,\n",
       "     'n_samples': 114,\n",
       "     'gain': 0}}}},\n",
       " 'sr': {'y_pred': 1,\n",
       "  'y_prob': 258.6007677543186,\n",
       "  'level': 1,\n",
       "  'split': [0, 21.0],\n",
       "  'n_samples': 7292,\n",
       "  'gain': 7392.920792160614,\n",
       "  'sl': {'y_pred': 1,\n",
       "   'y_prob': 287.7910901113736,\n",
       "   'level': 2,\n",
       "   'split': [1, 2.0],\n",
       "   'n_samples': 5924,\n",
       "   'gain': 8304.11949599313,\n",
       "   'sl': {'y_pred': 1,\n",
       "    'y_prob': 175.89256756756757,\n",
       "    'level': 3,\n",
       "    'split': [0, 16.0],\n",
       "    'n_samples': 1478,\n",
       "    'gain': 1134.7125713419155,\n",
       "    'sl': {'y_pred': 1,\n",
       "     'y_prob': 156.91318681318683,\n",
       "     'level': 4,\n",
       "     'split': [0, 9.0],\n",
       "     'n_samples': 908,\n",
       "     'gain': 2624.6681854894996,\n",
       "     'sl': {'y_pred': 1,\n",
       "      'y_prob': 248.94782608695652,\n",
       "      'level': 5,\n",
       "      'split': -1,\n",
       "      'n_samples': 113,\n",
       "      'gain': 0},\n",
       "     'sr': {'y_pred': 1,\n",
       "      'y_prob': 143.2409033877039,\n",
       "      'level': 5,\n",
       "      'split': [0, 12.0],\n",
       "      'n_samples': 795,\n",
       "      'gain': 439.6561657092534,\n",
       "      'sl': {'y_pred': 1,\n",
       "       'y_prob': 125.66568914956012,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 339,\n",
       "       'gain': 786.1489632008015},\n",
       "      'sr': {'y_pred': 1,\n",
       "       'y_prob': 155.70305676855895,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 456,\n",
       "       'gain': 4.874220914134639}}},\n",
       "    'sr': {'y_pred': 1,\n",
       "     'y_prob': 205.4737762237762,\n",
       "     'level': 4,\n",
       "     'split': [0, 19.0],\n",
       "     'n_samples': 570,\n",
       "     'gain': 4157.134528572889,\n",
       "     'sl': {'y_pred': 1,\n",
       "      'y_prob': 242.00581395348837,\n",
       "      'level': 5,\n",
       "      'split': [0, 17.0],\n",
       "      'n_samples': 342,\n",
       "      'gain': 3000.6049382716155,\n",
       "      'sl': {'y_pred': 1,\n",
       "       'y_prob': 185.39655172413794,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 114,\n",
       "       'gain': 0},\n",
       "      'sr': {'y_pred': 1,\n",
       "       'y_prob': 268.45652173913044,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 228,\n",
       "       'gain': 320.00061557401204}},\n",
       "     'sr': {'y_pred': 1,\n",
       "      'y_prob': 149.05217391304348,\n",
       "      'level': 5,\n",
       "      'split': [0, 20.0],\n",
       "      'n_samples': 228,\n",
       "      'gain': 1347.89785318559,\n",
       "      'sl': {'y_pred': 1,\n",
       "       'y_prob': 173.2844827586207,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 114,\n",
       "       'gain': 0},\n",
       "      'sr': {'y_pred': 1,\n",
       "       'y_prob': 122.25862068965517,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 114,\n",
       "       'gain': 0}}}},\n",
       "   'sr': {'y_pred': 1,\n",
       "    'y_prob': 324.8943345323741,\n",
       "    'level': 3,\n",
       "    'split': [0, 16.0],\n",
       "    'n_samples': 4446,\n",
       "    'gain': 7126.654106105911,\n",
       "    'sl': {'y_pred': 1,\n",
       "     'y_prob': 277.645726807889,\n",
       "     'level': 4,\n",
       "     'split': [0, 9.0],\n",
       "     'n_samples': 2736,\n",
       "     'gain': 4188.375695612398,\n",
       "     'sl': {'y_pred': 1,\n",
       "      'y_prob': 396.6075581395349,\n",
       "      'level': 5,\n",
       "      'split': [1, 3.0],\n",
       "      'n_samples': 342,\n",
       "      'gain': 76.23025888297707,\n",
       "      'sl': {'y_pred': 1,\n",
       "       'y_prob': 383.4741379310345,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 114,\n",
       "       'gain': 0},\n",
       "      'sr': {'y_pred': 1,\n",
       "       'y_prob': 399.7869565217391,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 228,\n",
       "       'gain': 0.09618344099726528}},\n",
       "     'sr': {'y_pred': 1,\n",
       "      'y_prob': 260.3347245409015,\n",
       "      'level': 5,\n",
       "      'split': [0, 12.0],\n",
       "      'n_samples': 2394,\n",
       "      'gain': 1622.4360912724806,\n",
       "      'sl': {'y_pred': 1,\n",
       "       'y_prob': 227.22178988326849,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 1026,\n",
       "       'gain': 267.829494545309},\n",
       "      'sr': {'y_pred': 1,\n",
       "       'y_prob': 284.8021897810219,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 1368,\n",
       "       'gain': 123.14834104932379}}},\n",
       "    'sr': {'y_pred': 1,\n",
       "     'y_prob': 400.08002336448595,\n",
       "     'level': 4,\n",
       "     'split': [0, 19.0],\n",
       "     'n_samples': 1710,\n",
       "     'gain': 10357.058249717928,\n",
       "     'sl': {'y_pred': 1,\n",
       "      'y_prob': 458.41147859922177,\n",
       "      'level': 5,\n",
       "      'split': [0, 17.0],\n",
       "      'n_samples': 1026,\n",
       "      'gain': 10070.885409185605,\n",
       "      'sl': {'y_pred': 1,\n",
       "       'y_prob': 356.8662790697674,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 342,\n",
       "       'gain': 158.30348654292175},\n",
       "      'sr': {'y_pred': 1,\n",
       "       'y_prob': 507.99708454810497,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 684,\n",
       "       'gain': 2829.337849680276}},\n",
       "     'sr': {'y_pred': 1,\n",
       "      'y_prob': 311.50291545189503,\n",
       "      'level': 5,\n",
       "      'split': [0, 20.0],\n",
       "      'n_samples': 684,\n",
       "      'gain': 4838.748401217395,\n",
       "      'sl': {'y_pred': 1,\n",
       "       'y_prob': 359.5,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 342,\n",
       "       'gain': 3873.788481926109},\n",
       "      'sr': {'y_pred': 1,\n",
       "       'y_prob': 261.69767441860466,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 342,\n",
       "       'gain': 2368.159852604207}}}}},\n",
       "  'sr': {'y_pred': 1,\n",
       "   'y_prob': 131.95985401459853,\n",
       "   'level': 2,\n",
       "   'split': [1, 2.0],\n",
       "   'n_samples': 1368,\n",
       "   'gain': 2478.289069001301,\n",
       "   'sl': {'y_pred': 1,\n",
       "    'y_prob': 70.7703488372093,\n",
       "    'level': 3,\n",
       "    'split': [0, 23.0],\n",
       "    'n_samples': 342,\n",
       "    'gain': 668.1166854758721,\n",
       "    'sl': {'y_pred': 1,\n",
       "     'y_prob': 83.37826086956522,\n",
       "     'level': 4,\n",
       "     'split': [0, 22.0],\n",
       "     'n_samples': 228,\n",
       "     'gain': 250.5657125269281,\n",
       "     'sl': {'y_pred': 1,\n",
       "      'y_prob': 93.66379310344827,\n",
       "      'level': 5,\n",
       "      'split': -1,\n",
       "      'n_samples': 114,\n",
       "      'gain': 0},\n",
       "     'sr': {'y_pred': 1,\n",
       "      'y_prob': 71.66379310344827,\n",
       "      'level': 5,\n",
       "      'split': -1,\n",
       "      'n_samples': 114,\n",
       "      'gain': 0}},\n",
       "    'sr': {'y_pred': 1,\n",
       "     'y_prob': 44.560344827586206,\n",
       "     'level': 4,\n",
       "     'split': -1,\n",
       "     'n_samples': 114,\n",
       "     'gain': 0}},\n",
       "   'sr': {'y_pred': 1,\n",
       "    'y_prob': 152.17996108949416,\n",
       "    'level': 3,\n",
       "    'split': [0, 23.0],\n",
       "    'n_samples': 1026,\n",
       "    'gain': 2327.263104696969,\n",
       "    'sl': {'y_pred': 1,\n",
       "     'y_prob': 176.08309037900875,\n",
       "     'level': 4,\n",
       "     'split': [0, 22.0],\n",
       "     'n_samples': 684,\n",
       "     'gain': 1039.5200061557407,\n",
       "     'sl': {'y_pred': 1,\n",
       "      'y_prob': 198.23837209302326,\n",
       "      'level': 5,\n",
       "      'split': [1, 4.0],\n",
       "      'n_samples': 342,\n",
       "      'gain': 1333.531009541417,\n",
       "      'sl': {'y_pred': 1,\n",
       "       'y_prob': 215.76521739130436,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 228,\n",
       "       'gain': 674.4755694059422},\n",
       "      'sr': {'y_pred': 1,\n",
       "       'y_prob': 160.07758620689654,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 114,\n",
       "       'gain': 0}},\n",
       "     'sr': {'y_pred': 1,\n",
       "      'y_prob': 152.90697674418604,\n",
       "      'level': 5,\n",
       "      'split': [1, 4.0],\n",
       "      'n_samples': 342,\n",
       "      'gain': 749.9910741766653,\n",
       "      'sl': {'y_pred': 1,\n",
       "       'y_prob': 166.0391304347826,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 228,\n",
       "       'gain': 353.68667282239767},\n",
       "      'sr': {'y_pred': 1,\n",
       "       'y_prob': 124.24137931034483,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 114,\n",
       "       'gain': 0}}},\n",
       "    'sr': {'y_pred': 1,\n",
       "     'y_prob': 103.63081395348837,\n",
       "     'level': 4,\n",
       "     'split': [1, 4.0],\n",
       "     'n_samples': 342,\n",
       "     'gain': 223.0743134639706,\n",
       "     'sl': {'y_pred': 1,\n",
       "      'y_prob': 110.73478260869565,\n",
       "      'level': 5,\n",
       "      'split': [1, 3.0],\n",
       "      'n_samples': 228,\n",
       "      'gain': 200.70236995998494,\n",
       "      'sl': {'y_pred': 1,\n",
       "       'y_prob': 99.9396551724138,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 114,\n",
       "       'gain': 0},\n",
       "      'sr': {'y_pred': 1,\n",
       "       'y_prob': 119.62931034482759,\n",
       "       'level': 6,\n",
       "       'split': -1,\n",
       "       'n_samples': 114,\n",
       "       'gain': 0}},\n",
       "     'sr': {'y_pred': 1,\n",
       "      'y_prob': 87.76724137931035,\n",
       "      'level': 5,\n",
       "      'split': -1,\n",
       "      'n_samples': 114,\n",
       "      'gain': 0}}}}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construir el árbol de decisiones\n",
    "tree_grow(X, y, level=0, min_gain=0.001, max_depth=6, num_pct=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 5 - Árbol de decisión con librería\n",
    "En la celda 5 entrene un árbol de decisiones con la **librería sklearn**, usando las variables predictoras \"season\" y \"hour\" y calibre los parámetros que considere conveniente para obtener un mejor desempeño. Recuerde dividir los datos en conjuntos de entrenamiento y validación para esto. Comente el desempeño del modelo con alguna métrica de desempeño de modelos de regresión y compare desempeño con el modelo del punto 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 ajustado árbol de decisión: 0.5761010614049551\n",
      "R^2 ajustado Regresión lienal: 0.2231079081517925\n"
     ]
    }
   ],
   "source": [
    "# Celda 5\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "tree_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en el conjunto de validación\n",
    "y_pred = tree_model.predict(X_test)\n",
    "\n",
    "# Entrenar el modelo\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en el conjunto de validación\n",
    "y_pred = tree_model.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Número de observaciones\n",
    "n = len(y_test)\n",
    "\n",
    "# Número de variables predictoras\n",
    "p = X_test.shape[1]\n",
    "\n",
    "# Calcula el R^2 ajustado\n",
    "r2_adjusted_punto5 = 1 - (1 - r2) * ((n - 1) / (n - p - 1))\n",
    "\n",
    "print(\"R^2 ajustado árbol de decisión:\", r2_adjusted_punto5)\n",
    "print(\"R^2 ajustado Regresión lienal:\", r2_adjusted_punto3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El árbol de decisión tiene un R^2 ajustado de aproximadamente 0.576, lo que significa que alrededor del 57.6% de la variabilidad en la variable de respuesta es explicada por el modelo. Mientras que la regresión lineal tiene un R^2 ajustado de aproximadamente 0.223, lo que indica que solo alrededor del 22.3% de la variabilidad es explicada por el modelo lineal. por lo que basándose en los valores de R^2 ajustado proporcionados, se puede concluir que el modelo de árbol de decisión tiene un mejor rendimiento en este conjunto de datos en comparación con el modelo de regresión lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte B - Métodos de ensamblajes\n",
    "En esta parte del taller se usará el conjunto de datos de Popularidad de Noticias Online. El objetivo es predecir si la notica es popular o no, la popularidad está dada por la cantidad de reacciones en redes sociales. Para más detalles puede visitar el siguiente enlace: [datos](https://archive.ics.uci.edu/ml/datasets/online+news+popularity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos popularidad de noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>Popular</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2014/12/10/cia-torture-rep...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>0.732620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.844262</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.487500</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/10/18/bitlock-kicksta...</td>\n",
       "      <td>447.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>0.653199</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.135340</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/07/24/google-glass-po...</td>\n",
       "      <td>533.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.775701</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/11/21/these-are-the-m...</td>\n",
       "      <td>413.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.497409</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.677350</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.195701</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2014/02/11/parking-ticket-...</td>\n",
       "      <td>331.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.830357</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  timedelta  \\\n",
       "0  http://mashable.com/2014/12/10/cia-torture-rep...       28.0   \n",
       "1  http://mashable.com/2013/10/18/bitlock-kicksta...      447.0   \n",
       "2  http://mashable.com/2013/07/24/google-glass-po...      533.0   \n",
       "3  http://mashable.com/2013/11/21/these-are-the-m...      413.0   \n",
       "4  http://mashable.com/2014/02/11/parking-ticket-...      331.0   \n",
       "\n",
       "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0             9.0             188.0         0.732620               1.0   \n",
       "1             7.0             297.0         0.653199               1.0   \n",
       "2            11.0             181.0         0.660377               1.0   \n",
       "3            12.0             781.0         0.497409               1.0   \n",
       "4             8.0             177.0         0.685714               1.0   \n",
       "\n",
       "   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  ...  \\\n",
       "0                  0.844262        5.0             1.0       1.0  ...   \n",
       "1                  0.815789        9.0             4.0       1.0  ...   \n",
       "2                  0.775701        4.0             3.0       1.0  ...   \n",
       "3                  0.677350       10.0             3.0       1.0  ...   \n",
       "4                  0.830357        3.0             2.0       1.0  ...   \n",
       "\n",
       "   min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
       "0               0.200000                   0.80              -0.487500   \n",
       "1               0.160000                   0.50              -0.135340   \n",
       "2               0.136364                   1.00               0.000000   \n",
       "3               0.100000                   1.00              -0.195701   \n",
       "4               0.100000                   0.55              -0.175000   \n",
       "\n",
       "   min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "0                  -0.60              -0.250000                 0.9   \n",
       "1                  -0.40              -0.050000                 0.1   \n",
       "2                   0.00               0.000000                 0.3   \n",
       "3                  -0.40              -0.071429                 0.0   \n",
       "4                  -0.25              -0.100000                 0.0   \n",
       "\n",
       "   title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                       0.8                     0.4   \n",
       "1                      -0.1                     0.4   \n",
       "2                       1.0                     0.2   \n",
       "3                       0.0                     0.5   \n",
       "4                       0.0                     0.5   \n",
       "\n",
       "   abs_title_sentiment_polarity  Popular  \n",
       "0                           0.8        1  \n",
       "1                           0.1        0  \n",
       "2                           1.0        0  \n",
       "3                           0.0        0  \n",
       "4                           0.0        0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura de la información de archivo .csv\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2023/main/datasets/mashable.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definición variable de interes y variables predictoras\n",
    "X = df.drop(['url', 'Popular'], axis=1)\n",
    "y = df['Popular']\n",
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División de la muestra en set de entrenamiento y prueba\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 6 - Árbol de decisión y regresión logística\n",
    "En la celda 6 construya un árbol de decisión y una regresión logística. Para el árbol calibre al menos un parámetro y evalúe el desempeño de cada modelo usando las métricas de Accuracy y F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor parámetro para el árbol de decisión: {'max_depth': 4}\n",
      "Accuracy del árbol: 0.6513333333333333\n",
      "F1-score del árbol: 0.6345213137665969\n",
      "Accuracy de regresión logistica: 0.614\n",
      "F1-score de regresión logistica: 0.6106254203093476\n"
     ]
    }
   ],
   "source": [
    "# Celda 6\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'max_depth': range(1, 20)}  \n",
    "\n",
    "# Inicializar el clasificador de árbol de decisión\n",
    "tree_model = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "grid_search = GridSearchCV(tree_model, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Entrenar GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor modelo después de la calibración\n",
    "best_tree_model = grid_search.best_estimator_\n",
    "\n",
    "# Predecir utilizando el mejor modelo\n",
    "y_pred_tree = best_tree_model.predict(X_test)\n",
    "\n",
    "# Calcular métricas de evaluación\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "f1_tree = f1_score(y_test, y_pred_tree)\n",
    "\n",
    "\n",
    "# # Regresion logistica\n",
    "log_reg_model = LogisticRegression()\n",
    "\n",
    "# # Entrenamiento regresion logistica\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "y_pred_RL = log_reg_model.predict(X_test)\n",
    "accuracy_RL = accuracy_score(y_test, y_pred_RL)\n",
    "f1_RL = f1_score(y_test, y_pred_RL)\n",
    "\n",
    "print(\"Mejor parámetro para el árbol de decisión:\", grid_search.best_params_)\n",
    "print(\"Accuracy del árbol:\", accuracy_tree)\n",
    "print(\"F1-score del árbol:\", f1_tree)\n",
    "print(\"Accuracy de regresión logistica:\", accuracy_RL)\n",
    "print(\"F1-score de regresión logistica:\", f1_RL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se observa que el árbol de decisión tiene una mejor precisión general en la clasificación de las muestras en comparación con el modelo de regresión logística.\n",
    "* El mel árbol de decisión parece tener un mejor desempeño que el modelo de regresión logística en este conjunto de datos específico, teniendo en cuenta ambas métricas de evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 7 - Votación Mayoritaria\n",
    "En la celda 7 elabore un esamble con la metodología de **Votación mayoritaria** compuesto por 300 muestras bagged donde:\n",
    "\n",
    "-las primeras 100 muestras vienen de árboles de decisión donde max_depth tome un valor de su elección\\\n",
    "-las segundas 100 muestras vienen de árboles de decisión donde min_samples_leaf tome un valor de su elección\\\n",
    "-las últimas 100 muestras vienen de regresiones logísticas\n",
    "\n",
    "Evalúe cada uno de los tres modelos de manera independiente utilizando las métricas de Accuracy y F1-Score, luego evalúe el ensamble de modelos y compare los resultados. \n",
    "\n",
    "Nota: \n",
    "\n",
    "Para este ensamble de 300 modelos, deben hacer votación mayoritaria. Esto lo pueden hacer de distintas maneras. La más \"fácil\" es haciendo la votación \"manualmente\", como se hace a partir del minuto 5:45 del video de Ejemplo práctico de emsablajes en Coursera. Digo que es la más fácil porque si hacen la votación mayoritaria sobre las 300 predicciones van a obtener lo que se espera.\n",
    "\n",
    "Otra opción es: para cada uno de los 3 tipos de modelos, entrenar un ensamble de 100 modelos cada uno. Predecir para cada uno de esos tres ensambles y luego predecir como un ensamble de los 3 ensambles. La cuestión es que la votación mayoritaria al usar los 3 ensambles no necesariamente va a generar el mismo resultado que si hacen la votación mayoritaria directamente sobre los 300 modelos. Entonces, para los que quieran hacer esto, deben hacer ese último cálculo con cuidado.\n",
    "\n",
    "Para los que quieran hacerlo como ensamble de ensambles, digo que se debe hacer el ensamble final con cuidado por lo siguiente. Supongamos que:\n",
    "\n",
    "* para los 100 árboles del primer tipo, la votación mayoritaria es: 55% de los modelos predicen que la clase de una observación es \"1\"\n",
    "* para los 100 árboles del segundo tipo, la votación mayoritaria es: 55% de los modelos predicen que la clase de una observación es \"1\"\n",
    "* para las 100 regresiones logísticas, la votación mayoritaria es: 10% de los modelos predicen que la clase de una observación es \"1\"\n",
    "\n",
    "Si se hace la votación mayoritaria de los 300 modelos, la predicción de esa observación debería ser: (100*55%+100*55%+100*10%)/300 = 40% de los modelos votan porque la predicción debería ser \"1\". Es decir, la predicción del ensamble es \"0\" (dado que menos del 50% de modelos predijo un 1).\n",
    "\n",
    "Sin embargo, si miramos cada ensamble por separado, el primer ensamble predice \"1\", el segundo ensamble predice \"1\" y el último ensamble predice \"0\". Si hago votación mayoritaria sobre esto, la predicción va a ser \"1\", lo cual es distinto a si se hace la votación mayoritaria sobre los 300 modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de evaluación para el subconjunto de árboles de decisión con max_depth específico:\n",
      "Accuracy: 0.6513333333333333\n",
      "F1-score: 0.6345213137665969\n",
      "\n",
      "Métricas de evaluación para el subconjunto de árboles de decisión con min_samples_leaf específico:\n",
      "Accuracy: 0.57\n",
      "F1-score: 0.551772063933287\n",
      "\n",
      "Métricas de evaluación para el subconjunto de regresión logística:\n",
      "Accuracy: 0.614\n",
      "F1-score: 0.6106254203093476\n",
      "\n",
      "Métricas de evaluación para el ensamble final:\n",
      "Accuracy del ensamble: 0.642\n",
      "F1-score del ensamble: 0.629399585921325\n"
     ]
    }
   ],
   "source": [
    "# Celda 7\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# árboles de decisión con max_depth específico\n",
    "tree_models_max_depth = []\n",
    "for _ in range(100):\n",
    "    tree_model_max_depth = DecisionTreeClassifier(max_depth=4) \n",
    "    tree_model_max_depth.fit(X_train, y_train)\n",
    "    tree_models_max_depth.append(tree_model_max_depth)\n",
    "\n",
    "#  árboles de decisión con min_samples_leaf específico\n",
    "tree_models_min_samples_leaf = []\n",
    "for _ in range(100):\n",
    "    tree_model_min_samples_leaf = DecisionTreeClassifier(min_samples_leaf=20)  \n",
    "    tree_model_min_samples_leaf.fit(X_train, y_train)\n",
    "    tree_models_min_samples_leaf.append(tree_model_min_samples_leaf)\n",
    "\n",
    "# regresión logística\n",
    "log_reg_models = []\n",
    "for _ in range(100):\n",
    "    log_reg_model = LogisticRegression()\n",
    "    log_reg_model.fit(X_train, y_train)\n",
    "    log_reg_models.append(log_reg_model)\n",
    "\n",
    "# Generar predicciones para cada subconjunto de modelos\n",
    "predictions_max_depth = [model.predict(X_test) for model in tree_models_max_depth]\n",
    "predictions_min_samples_leaf = [model.predict(X_test) for model in tree_models_min_samples_leaf]\n",
    "predictions_log_reg = [model.predict(X_test) for model in log_reg_models]\n",
    "\n",
    "# Realizar la votación mayoritaria para cada subconjunto de modelos\n",
    "ensemble_predictions_max_depth = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=predictions_max_depth)\n",
    "ensemble_predictions_min_samples_leaf = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=predictions_min_samples_leaf)\n",
    "ensemble_predictions_log_reg = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=predictions_log_reg)\n",
    "\n",
    "# Evaluar cada subconjunto de modelos\n",
    "accuracy_tree_max_depth = accuracy_score(y_test, ensemble_predictions_max_depth)\n",
    "f1_tree_max_depth = f1_score(y_test, ensemble_predictions_max_depth)\n",
    "accuracy_tree_min_samples_leaf = accuracy_score(y_test, ensemble_predictions_min_samples_leaf)\n",
    "f1_tree_min_samples_leaf = f1_score(y_test, ensemble_predictions_min_samples_leaf)\n",
    "accuracy_log_reg = accuracy_score(y_test, ensemble_predictions_log_reg)\n",
    "f1_log_reg = f1_score(y_test, ensemble_predictions_log_reg)\n",
    "\n",
    "# Realizar la votación mayoritaria para el ensamble final\n",
    "final_predictions = np.column_stack((ensemble_predictions_max_depth, ensemble_predictions_min_samples_leaf, ensemble_predictions_log_reg))\n",
    "ensemble_predictions = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=1, arr=final_predictions)\n",
    "\n",
    "######3 Evaluar el ensamble final\n",
    "accuracy_ensemble = accuracy_score(y_test, ensemble_predictions)\n",
    "f1_ensemble = f1_score(y_test, ensemble_predictions)\n",
    "\n",
    "# métricas de evaluación\n",
    "print(\"Métricas de evaluación para el subconjunto de árboles de decisión con max_depth específico:\")\n",
    "print(\"Accuracy:\", accuracy_tree_max_depth)\n",
    "print(\"F1-score:\", f1_tree_max_depth)\n",
    "print(\"\\nMétricas de evaluación para el subconjunto de árboles de decisión con min_samples_leaf específico:\")\n",
    "print(\"Accuracy:\", accuracy_tree_min_samples_leaf)\n",
    "print(\"F1-score:\", f1_tree_min_samples_leaf)\n",
    "print(\"\\nMétricas de evaluación para el subconjunto de regresión logística:\")\n",
    "print(\"Accuracy:\", accuracy_log_reg)\n",
    "print(\"F1-score:\", f1_log_reg)\n",
    "print(\"\\nMétricas de evaluación para el ensamble final:\")\n",
    "print(\"Accuracy del ensamble:\", accuracy_ensemble)\n",
    "print(\"F1-score del ensamble:\", f1_ensemble)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se observa que el subconjunto de árboles de decisión con max_depth específico tiene el mejor desempeño en términos de precisión (Accuracy) y F1-score en comparación con los otros dos subconjuntos de modelos.\n",
    "* El subconjunto de árboles de decisión con min_samples_leaf específico tiene el peor desempeño, con una precisión y F1-score más bajos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 8 - Votación Ponderada\n",
    "En la celda 8 elabore un ensamble con la metodología de **Votación ponderada** compuesto por 300 muestras bagged para los mismos tres escenarios del punto 7. Evalúe los modelos utilizando las métricas de Accuracy y F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 9 - Comparación y análisis de resultados\n",
    "En la celda 9 comente sobre los resultados obtenidos con las metodologías usadas en los puntos 7 y 8, compare los resultados y enuncie posibles ventajas o desventajas de cada una de ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
